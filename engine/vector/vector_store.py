"""
Vector Store ‚Äî —Ö—Ä–∞–Ω–∏–ª–∏—â–µ embeddings –≤ SQLite.

–•—Ä–∞–Ω–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏ + –∏—Ö –≤–µ–∫—Ç–æ—Ä–∞.
–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ cosine similarity (numpy, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π).

–î–ª—è vault –¥–æ ~10K –∑–∞–º–µ—Ç–æ–∫ —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.
–ü—Ä–∏ –º–∞—Å—à—Ç–∞–±–µ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ FAISS/pgvector.
"""

import json
import sqlite3
import numpy as np
from dataclasses import dataclass
from typing import Optional

from engine.vector.embedder import Embedder
from engine.parser.markdown_parser import ParsedNote, parse_vault


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    chunk_id: str
    entity_id: str
    entity_name: str
    section: str
    content: str
    score: float

    def __repr__(self):
        preview = self.content[:60] + "..." if len(self.content) > 60 else self.content
        return f"Result({self.score:.3f} | {self.entity_name}/{self.section}: {preview})"


class VectorStore:
    """SQLite-based Vector Store —Å cosine similarity search"""

    def __init__(self, db_path: str = ":memory:", embedder: Optional[Embedder] = None):
        self.db_path = db_path
        self.embedder = embedder or Embedder()
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self._create_tables()

        # In-memory –∫–µ—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self._vectors: Optional[np.ndarray] = None
        self._chunk_ids: list[str] = []

    def _create_tables(self):
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS chunks (
                id TEXT PRIMARY KEY,
                entity_id TEXT NOT NULL,
                entity_name TEXT NOT NULL,
                section TEXT NOT NULL,
                content TEXT NOT NULL,
                embedding BLOB NOT NULL,
                position INTEGER,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE INDEX IF NOT EXISTS idx_chunks_entity ON chunks(entity_id);
        """)
        self.conn.commit()

    def add_chunk(self, chunk_id: str, entity_id: str, entity_name: str,
                  section: str, content: str, position: int = 0):
        """–î–æ–±–∞–≤–∏—Ç—å –æ–¥–∏–Ω —á–∞–Ω–∫ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π embedding"""
        vector = self.embedder.embed(content)
        self.conn.execute(
            """INSERT OR REPLACE INTO chunks 
               (id, entity_id, entity_name, section, content, embedding, position)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (chunk_id, entity_id, entity_name, section, content,
             vector.tobytes(), position),
        )
        self.conn.commit()
        self._invalidate_cache()

    def add_chunks_batch(self, chunks: list[dict]):
        """
        –ë–∞—Ç—á-–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (–±—ã—Å—Ç—Ä–µ–µ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏).
        chunks: [{chunk_id, entity_id, entity_name, section, content, position}]
        """
        if not chunks:
            return

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ embeddings –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
        texts = [c["content"] for c in chunks]
        vectors = self.embedder.embed_batch(texts)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –ë–î
        rows = [
            (c["chunk_id"], c["entity_id"], c["entity_name"],
             c["section"], c["content"], vectors[i].tobytes(), c.get("position", 0))
            for i, c in enumerate(chunks)
        ]
        self.conn.executemany(
            """INSERT OR REPLACE INTO chunks 
               (id, entity_id, entity_name, section, content, embedding, position)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            rows,
        )
        self.conn.commit()
        self._invalidate_cache()
        print(f"   ‚úÖ Indexed {len(chunks)} chunks")

    def search(self, query: str, top_k: int = 5, min_score: float = 0.0) -> list[SearchResult]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç top_k —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
        """
        self._ensure_cache()

        if len(self._chunk_ids) == 0:
            return []

        # Embed –∑–∞–ø—Ä–æ—Å
        query_vec = self.embedder.embed(query)

        # Cosine similarity (–≤–µ–∫—Ç–æ—Ä–∞ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        scores = np.dot(self._vectors, query_vec)

        # Top-K
        top_indices = np.argsort(scores)[::-1][:top_k]

        results = []
        for idx in top_indices:
            score = float(scores[idx])
            if score < min_score:
                break

            chunk_id = self._chunk_ids[idx]
            row = self.conn.execute(
                "SELECT * FROM chunks WHERE id = ?", (chunk_id,)
            ).fetchone()

            if row:
                results.append(SearchResult(
                    chunk_id=row["id"],
                    entity_id=row["entity_id"],
                    entity_name=row["entity_name"],
                    section=row["section"],
                    content=row["content"],
                    score=score,
                ))

        return results

    def search_by_entity(self, entity_id: str) -> list[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π entity"""
        rows = self.conn.execute(
            "SELECT * FROM chunks WHERE entity_id = ? ORDER BY position",
            (entity_id,),
        ).fetchall()
        return [dict(r) for r in rows]

    def stats(self) -> dict:
        total = self.conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
        entities = self.conn.execute(
            "SELECT COUNT(DISTINCT entity_id) FROM chunks"
        ).fetchone()[0]
        return {"total_chunks": total, "total_entities": entities}

    def _ensure_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä–∞ –≤ RAM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        if self._vectors is not None:
            return

        rows = self.conn.execute("SELECT id, embedding FROM chunks").fetchall()
        if not rows:
            self._vectors = np.array([])
            self._chunk_ids = []
            return

        self._chunk_ids = [r["id"] for r in rows]
        dim = self.embedder.dimensions
        self._vectors = np.array([
            np.frombuffer(r["embedding"], dtype=np.float32)
            for r in rows
        ])

    def _invalidate_cache(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∫–µ—à –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self._vectors = None
        self._chunk_ids = []

    def close(self):
        self.conn.close()


def index_vault(vault_path: str, db_path: str = ":memory:") -> VectorStore:
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤–µ—Å—å Obsidian vault –≤ Vector Store.
    """
    notes = parse_vault(vault_path)
    store = VectorStore(db_path)

    print(f"üìù –ò–Ω–¥–µ–∫—Å–∏—Ä—É—é {len(notes)} –∑–∞–º–µ—Ç–æ–∫...")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
    all_chunks = []
    for note in notes:
        entity_id = note.name.lower().replace(" ", "_")
        for chunk in note.chunks:
            all_chunks.append({
                "chunk_id": f"{entity_id}:{chunk.position}",
                "entity_id": entity_id,
                "entity_name": note.name,
                "section": chunk.section,
                "content": chunk.content,
                "position": chunk.position,
            })

    # –ë–∞—Ç—á-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    store.add_chunks_batch(all_chunks)

    stats = store.stats()
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {stats['total_chunks']} —á–∞–Ω–∫–æ–≤ –∏–∑ {stats['total_entities']} –∑–∞–º–µ—Ç–æ–∫")

    return store


if __name__ == "__main__":
    import sys

    vault_path = sys.argv[1] if len(sys.argv) > 1 else "./test_vault"
    store = index_vault(vault_path)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "–ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö",
        "–∫—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ backend",
        "–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ Redis",
        "–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞",
    ]

    for q in queries:
        print(f"\nüîç Query: '{q}'")
        results = store.search(q, top_k=3)
        for r in results:
            print(f"   {r.score:.3f} | {r.entity_name}/{r.section}")
            print(f"           {r.content[:80]}...")

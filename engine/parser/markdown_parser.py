"""
Obsidian Markdown Parser

–ü–∞—Ä—Å–∏—Ç .md —Ñ–∞–π–ª—ã –∏–∑ Obsidian vault –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç:
- Frontmatter (YAML metadata)
- Wikilinks ([[links]])
- Tags (#tags –∏ –∏–∑ frontmatter)
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
- –¢–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏ (–¥–ª—è embeddings)
"""

import re
import yaml
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional


@dataclass
class WikiLink:
    """–°—Å—ã–ª–∫–∞ [[target]] –∏–ª–∏ [[target|alias]]"""
    target: str
    alias: Optional[str] = None
    context: str = ""  # –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞

    def __repr__(self):
        if self.alias:
            return f"WikiLink({self.target} | {self.alias})"
        return f"WikiLink({self.target})"


@dataclass
class Section:
    """–°–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∫–æ–Ω—Ç–µ–Ω—Ç)"""
    title: str
    level: int  # 1 = #, 2 = ##, 3 = ###
    content: str

    def __repr__(self):
        return f"Section(L{self.level}: {self.title})"


@dataclass
class TextChunk:
    """–ß–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è vector embedding"""
    content: str
    section: str  # –≤ –∫–∞–∫–æ–π —Å–µ–∫—Ü–∏–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è
    position: int  # –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

    def __repr__(self):
        preview = self.content[:50] + "..." if len(self.content) > 50 else self.content
        return f"Chunk({self.section}: {preview})"


@dataclass
class ParsedNote:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–π .md –∑–∞–º–µ—Ç–∫–∏"""
    file_path: str
    title: str
    frontmatter: dict = field(default_factory=dict)
    tags: list[str] = field(default_factory=list)
    wikilinks: list[WikiLink] = field(default_factory=list)
    sections: list[Section] = field(default_factory=list)
    chunks: list[TextChunk] = field(default_factory=list)
    raw_content: str = ""

    @property
    def name(self) -> str:
        """–ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è = –∏–º—è —Å—É—â–Ω–æ—Å—Ç–∏"""
        return Path(self.file_path).stem

    def __repr__(self):
        return (
            f"ParsedNote(\n"
            f"  title={self.title}\n"
            f"  tags={self.tags}\n"
            f"  links={[l.target for l in self.wikilinks]}\n"
            f"  sections={len(self.sections)}\n"
            f"  chunks={len(self.chunks)}\n"
            f")"
        )


# Regex patterns
FRONTMATTER_RE = re.compile(r"^---\s*\n(.*?)\n---\s*\n", re.DOTALL)
WIKILINK_RE = re.compile(r"\[\[([^\]|]+)(?:\|([^\]]+))?\]\]")
TAG_RE = re.compile(r"(?:^|\s)#([a-zA-Z\w\-/]+)", re.MULTILINE)
HEADING_RE = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)


def parse_frontmatter(content: str) -> tuple[dict, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç YAML frontmatter –∏–∑ –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (metadata_dict, content_without_frontmatter)
    """
    match = FRONTMATTER_RE.match(content)
    if not match:
        return {}, content

    try:
        metadata = yaml.safe_load(match.group(1)) or {}
    except yaml.YAMLError:
        metadata = {}

    body = content[match.end():]
    return metadata, body


def extract_wikilinks(content: str) -> list[WikiLink]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ [[wikilinks]] –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç [[target]] –∏ [[target|alias]] —Ñ–æ—Ä–º–∞—Ç—ã.
    """
    links = []
    for match in WIKILINK_RE.finditer(content):
        target = match.group(1).strip()
        alias = match.group(2).strip() if match.group(2) else None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–∫–∏
        start = max(0, match.start() - 80)
        end = min(len(content), match.end() + 80)
        context = content[start:end].replace("\n", " ").strip()

        links.append(WikiLink(target=target, alias=alias, context=context))

    return links


def extract_tags(content: str, frontmatter: dict) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏ –∏–∑:
    1. Frontmatter (tags: [tag1, tag2])
    2. Inline #tags –≤ —Ç–µ–∫—Å—Ç–µ
    """
    tags = set()

    # –ò–∑ frontmatter
    fm_tags = frontmatter.get("tags", [])
    if isinstance(fm_tags, list):
        tags.update(fm_tags)
    elif isinstance(fm_tags, str):
        tags.add(fm_tags)

    # Inline —Ç–µ–≥–∏
    for match in TAG_RE.finditer(content):
        tag = match.group(1)
        # –ò—Å–∫–ª—é—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (## –Ω–µ —Ç–µ–≥)
        if not tag.startswith("#"):
            tags.add(tag)

    return sorted(tags)


def extract_sections(content: str) -> list[Section]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º.
    """
    sections = []
    headings = list(HEADING_RE.finditer(content))

    if not headings:
        # –ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ ‚Äî –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ –æ–¥–Ω–∞ —Å–µ–∫—Ü–∏—è
        stripped = content.strip()
        if stripped:
            sections.append(Section(title="(root)", level=0, content=stripped))
        return sections

    # –¢–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    pre_heading = content[:headings[0].start()].strip()
    if pre_heading:
        sections.append(Section(title="(intro)", level=0, content=pre_heading))

    # –ö–∞–∂–¥—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ
    for i, heading in enumerate(headings):
        level = len(heading.group(1))
        title = heading.group(2).strip()

        # –ö–æ–Ω—Ç–µ–Ω—Ç: –æ—Ç –∫–æ–Ω—Ü–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ (–∏–ª–∏ –∫–æ–Ω—Ü–∞ —Ñ–∞–π–ª–∞)
        start = heading.end()
        end = headings[i + 1].start() if i + 1 < len(headings) else len(content)
        body = content[start:end].strip()

        sections.append(Section(title=title, level=level, content=body))

    return sections


def create_chunks(sections: list[Section], chunk_size: int = 500) -> list[TextChunk]:
    """
    –°–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è vector embeddings.
    –†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ —Å–µ–∫—Ü–∏—è–º, –¥–ª–∏–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ ‚Äî –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º.

    chunk_size ‚Äî –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö.
    """
    chunks = []
    position = 0

    for section in sections:
        if not section.content:
            continue

        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∞—è ‚Äî –æ–¥–∏–Ω —á–∞–Ω–∫
        if len(section.content) <= chunk_size:
            chunk_text = section.content
            if section.title not in ("(root)", "(intro)"):
                chunk_text = f"{section.title}: {chunk_text}"

            chunks.append(TextChunk(
                content=chunk_text,
                section=section.title,
                position=position,
            ))
            position += 1
            continue

        # –î–ª–∏–Ω–Ω–∞—è —Å–µ–∫—Ü–∏—è ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
        paragraphs = re.split(r"\n\s*\n", section.content)
        current_chunk = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            if len(current_chunk) + len(para) > chunk_size and current_chunk:
                chunks.append(TextChunk(
                    content=current_chunk.strip(),
                    section=section.title,
                    position=position,
                ))
                position += 1
                current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para

        if current_chunk.strip():
            chunks.append(TextChunk(
                content=current_chunk.strip(),
                section=section.title,
                position=position,
            ))
            position += 1

    return chunks


def parse_note(file_path: str) -> ParsedNote:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω .md —Ñ–∞–π–ª –≤ ParsedNote.
    """
    path = Path(file_path)
    content = path.read_text(encoding="utf-8")

    # 1. Frontmatter
    frontmatter, body = parse_frontmatter(content)

    # 2. Title ‚Äî –∏–∑ H1 –∏–ª–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    h1_match = re.search(r"^#\s+(.+)$", body, re.MULTILINE)
    title = h1_match.group(1).strip() if h1_match else path.stem

    # 3. Wikilinks
    wikilinks = extract_wikilinks(body)

    # 4. Tags
    tags = extract_tags(body, frontmatter)

    # 5. Sections
    sections = extract_sections(body)

    # 6. Chunks
    chunks = create_chunks(sections)

    return ParsedNote(
        file_path=str(path),
        title=title,
        frontmatter=frontmatter,
        tags=tags,
        wikilinks=wikilinks,
        sections=sections,
        chunks=chunks,
        raw_content=content,
    )


def parse_vault(vault_path: str) -> list[ParsedNote]:
    """
    –ü–∞—Ä—Å–∏—Ç –≤—Å–µ .md —Ñ–∞–π–ª—ã –≤ vault.
    """
    vault = Path(vault_path)
    notes = []

    for md_file in sorted(vault.rglob("*.md")):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã –∏ .obsidian –ø–∞–ø–∫—É
        if any(part.startswith(".") for part in md_file.parts):
            continue

        try:
            note = parse_note(str(md_file))
            notes.append(note)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {md_file}: {e}")

    return notes


# --- Entry point –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ---
if __name__ == "__main__":
    import sys

    vault_path = sys.argv[1] if len(sys.argv) > 1 else "./test_vault"
    notes = parse_vault(vault_path)

    print(f"\nüìö Parsed {len(notes)} notes from vault\n")

    for note in notes:
        print(f"{'='*60}")
        print(note)
        print(f"  frontmatter: {note.frontmatter}")
        print(f"  chunks preview:")
        for chunk in note.chunks[:2]:
            print(f"    - {chunk}")
        print()
